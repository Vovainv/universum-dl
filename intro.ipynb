{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добро пожаловать в мир дифференцируемых функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала небольшой ликбез, кто в школе ещё не проходил начала анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производная**. Интуитивно, производной называют скорость изменения функции. Её можно записать так:\n",
    "\n",
    "$$ f'(x) = \\frac{f(x+\\Delta x) - f(x)}{\\Delta x}, \\quad \\Delta x \\to 0 $$\n",
    "\n",
    "Её в основном считают, пользуясь этим определением. Например для функции $x^2$ она такая:\n",
    "\n",
    "$$ f'(x) = (x^2)' = \\frac{(x+\\Delta x)^2 - x^2}{\\Delta x} = \\frac{2x\\Delta x + \\Delta x^2}{\\Delta x} = 2x + \\Delta x \\to 2x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У производной очень много полезных свойств, позволяющих нам исследовать функции. Например, у нас есть такое свойство, что в точке экстремума (то есть минимума, максимума или «седловой точки») производная должна равняться нулю. Поэтому большинство простых задач на оптимизацию функции заключается в том, чтобы посчитать её производную (не фиксируя $x$), приравнять к нулю и посмотреть, в каких точках это равенство выполняется. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиент**. Что, если у нас фукция от нескольких переменных? В таком случае вводят обобщение производной — градиент. Это вектор (просто набор чисел), каждой компонентой которого является значение производной по очередному аргументу при фиксированных остальных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ок, зачем он нужен?** Пусть у нас есть какая-нибудь функция, которую мы хотим минимизировать, и у нас есть предположение, что она имеет вид какой-нибудь гладкой ямы. Тогда мы будем действовать так: начнем с какой-нибудь точки и будем делать много очень маленьких шажков в сторону наибольшего уменьшения функции, пока не придем в локальный минимум.\n",
    "\n",
    "* Что значит «в сторону наибольшего уменьшения»? Это значит «против градиаента».\n",
    "* Что значит «пока не придём в локальный минимум». Это значит «пока градиент не ноль»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grad_descent](images/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если learning rate достоаточно маленький, мы точно придем в локальный минимум. Но, возможно, не глобальный. Гарантированно находить глобальный минимум произвольной функции наука не умеет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как найти минимум такой функции? Давайте сделаем много маленьких шагов против градиента. Рано или поздно мы придем в минимум, хотя бы локальный.\n",
    "\n",
    "В принципе, мы бы могли считать градиент численно — просто сделать для каждого параметра это маленькие изменение и посмотреть, насколько стало лучше. Это возможно, но просто *безумно* долго: для каждого параметра нам нужно прогнать все ещё раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ок, зачем это надо?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим следующую «иерархию хороших функций»:\n",
    "\n",
    "* Аналитически решаемые, то есть минимум которых можно выразить какой-то простой формулой. Например, таким является линейная регрессия — можно взять производную, приравлять к нулю и выразить сразу все параметры.\n",
    "* Выпуклые. У них гарантируется решение, причём единственное. Его можно найти разными методами, в частности градиентным спуском, но обычно даже быстрее, чем спуском. Пример: логистическая регрессия.\n",
    "* Дифференцируемые. К ним можно применить градиентный спуск, но нужно очень много изворачиваться, чтобы он сошелся не к локальному минимуму, а к глобальному. **<-- YOU ARE HERE**\n",
    "* Дискретные. Тут всё грустно, но нам хотя бы можно быстро узнать её значение.\n",
    "* Невычислимые. Иногда нам нужно оценивать что-нибудь совсем не формализуемое математикой — например, качество перевода, или поведение пользователя. Невычислимыми функциями в частности занимается Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глубокое обучение — это про все методы, чьи функции потерь достаточно хорошие — то есть имеют производную."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для деревьев и линейных методов не обязательно понимать алгоритм, как они обучаются — это происходит вседа быстро. Для нейронных сетей это важно, потому что они самые общие, но и самые долгие для обучения. Иногда обучение нейросети может занимать по несколько недель на очень дорогом железе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В словосочетании «Deep Learning» автор понимает слово «Deep» как «непонятно, за что отвечает этот конкретный параметр» и как «дифференцируемое».\n",
    "\n",
    "Чтобы понимать DL, вам **нужно** сначала понять производные и градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Почему сейчас"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще, очень много чего было изобретено в 90е. Во-первых, было мало данных, во-вторых, вычислительных пощностей. С 1990 по 2010 «обычные», лежащие на полках магазинов GPU стали в 5000 раз быстрее, что позволило обучать нейросети людям без доступа к суперкомпьютерам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо железа и данных,\n",
    "* Лучшие функции активации. Когда-то использовали везде сигмоиду из-за её теорерной интерпретации, потом исследовали проблему затухающего градиента.\n",
    "* Лучшие функции потерь. Когда-то использовали везде MSE.\n",
    "* Техники инициализации весов. Инициализировать всё константой **очень** неправильно — все активации будут очень малыми. Сейчас инициализация работает так, чтобы \n",
    "* Фреймворки, позволяющие не считать градиенты самому и значительно упрощающие код. За 10 строчек в примере в конце тетрадки на самом деле стоят тысячи строк кода Keras и десятки тысяч строк кода более низкоуровневых библиотек, на котором в свою очередь строится он сам.\n",
    "* Улучшения градиентного спуска. Например, Adam и RMSProp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мотивация**. Нейросети отличаются от других методов тем, у них функции ошибки всегда «гладкие» — если чуть-чуть изменить значение отдельного параметра, то вывод сети почти не изменится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hierarchy](images/hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная\n",
    "\n",
    "Наш мозг является вдохновлением, а не объектом для реверс-инжениринга. По началу можно будет проводить аналогии с мозгом, но с какого-то момента модели будут настолько дикие, что ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](images/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ок, как их обучать?\n",
    "\n",
    "**Эта часть важна**. Достаточно простые шняги — линейная и логистическая регрессия, например — имеют аналитическое решение.\n",
    "\n",
    "Автор выделяет несколько классов «хороших» функций.\n",
    "\n",
    "* Аналитические решаемые. Например, таким является лосс в MSE — можно взять производную, приравлять к нулю и выразить сразу все параметры.\n",
    "* Выпуклые. У них гарантируется решение, причём единственное. Его можно найти разными методами, в частности градиентным спуском.\n",
    "* Дифференцируемые. К ним можно применить градиентный спуск, но нужно очень много изворачиваться, чтобы он сошелся не к локальному минимуму, а к глобальному. **<-- YOU ARE HERE**\n",
    "* Дискретные. Тут всё грустно, но нам хотя бы можно быстро узнать её значение.\n",
    "* Невычислимые. Иногда нам нужно оценивать что-нибудь совсем не формализуемое математикой — например, качество перевода, или поведение пользователя. Невычислимыми функциями в частности занимается Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производная**. Интуитивно, производной называют скорость изменения функции. Её можно записать так:\n",
    "\n",
    "$$ f'(x) = \\frac{f(x+\\Delta x) - f(x)}{\\Delta x}, \\quad \\Delta x \\to 0 $$\n",
    "\n",
    "Чуть формальнее вам объяснят на курсе матана. Её в основном считают, пользуясь этим определением. Например для функции $x^2$ она такая:\n",
    "\n",
    "$$ f'(x) = (x^2)' = \\frac{(x+\\Delta x)^2 - x^2}{\\Delta x} = \\frac{2x\\Delta x + \\Delta x^2}{\\Delta x} = 2x + \\Delta x \\to 2x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиент**. Градиентом какой-либо функции называют специальный вектор, составленный из производных по разным параметрам.\n",
    "\n",
    "Как найти минимум такой функции? Давайте сделаем много маленьких шагов против градиента. Рано или поздно мы придем в минимум, хотя бы локальный.\n",
    "\n",
    "В принципе, мы бы могли считать градиент численно — просто сделать для каждого параметра это маленькие изменение и посмотреть, насколько стало лучше. Это возможно, но просто *безумно* долго: для каждого параметра нам нужно прогнать все ещё раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Придумали способ посчитать градиент эффективно — за один прогон суммарно — основывающийся на следующем свойстве производной:\n",
    "\n",
    "$$ f(g(x))' = f'(g(x)) g'(x) $$\n",
    "\n",
    "TODO: доказать его"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как этот факт можно использовать? Вспомним, что сеть — это вычислительный граф. Как конкретный параметр влияет на функцию потерь? Ну, он куда-то дальше передается. Точнее, он передается последовательно в $n$ функций дальше:\n",
    "\n",
    "$$ f_x = f_1(f_2(\\ldots f_n(x))) $$\n",
    "\n",
    "Производной такой штуки будет, соответственно,\n",
    "\n",
    "$$ f'_x = f_1'(x) f_2(\\ldots f_n(x))) $$\n",
    "\n",
    "TODO: как-нибудь нормально записать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, мы можем *рекурсивно* посчитать градиент следующих вершин в графе, и после этого мы сразу можем посчитать градиент текущей вершины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grad_descent](images/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, это решение задачи «посчитай все производные» динамическим программированием, если кто занимался алгоритмами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросеть — это просто последовательность простых тензорных операций, у каждой из которых мы можем посчитать градиент.\n",
    "\n",
    "Алгоритм обратного распространения ошибки посдсчитывает все градиенты, начиная с конечного значения целевой функции. Он считает «вклад» каждого параметра.\n",
    "\n",
    "Не всегда inference будет работать так же как и train (и не всегда его можно будет посчитать за такое же время), но на первых занятиях это будет так."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастическая версия\n",
    "\n",
    "Обратим внимание, что мы делаем *маленький* шаг. Но данных у нас много. Чтобы для всех вычислить градиент, потребуется много ресурсов.\n",
    "\n",
    "Нам не обязательно считать все — достаточно взять какой-нибудь кусок данных (его называют batch) — и посчитать градиент только на нем. Градиент получается достаточно хорошим, если batch достаточно большой и репрезентативный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фреймворки\n",
    "\n",
    "У нас 12 дней, а не 6 лет, поэтому мы не будем вдаваться в подробности, как эти производные считаются аналитически. Это не очень сложно, но довольно трудоемко. Это сделали за нас.\n",
    "\n",
    "Все фреймворки сейчас разделяются на 2 типа: статические (TensorFlow, Theano) и динамические (PyTorch, Chainer). Также есть обертки над ними (Keras), которые упрощают жизнь и абстрагируют целые слои и процедуры.\n",
    "\n",
    "Выбор фреймворка — настоящая религиозная война. Автор пишет на PyTorch, но так как курс у нас не очень долгий, то будет писать все на Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Есть стандартный датасет. В нем 50000 картинок, по 5000 на каждую цифру от 0 до 9. Требуется распознать их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense # «плотный» слой — матричное умножение\n",
    "from keras.models import Sequential # вспомогательный класс, позволяющий последовательно выполнять операции\n",
    "from keras.optimizers import SGD # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist # датасет настолько известный, что он есть по умолчанию почти везде\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "(X, y), _ = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности формата: сейчас каждая картинка представляет собой трехмерные массивы (напомним, что многомерные массивы называют тензорами) размера n x 28 x 28. Мы хотим представить каждую как вектор размера $784 = 28^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна деталь: инициализация весов — отдельное искусство. В фреймворках оно опять же сделано за нас. Но сделано оно так, что предполагает, что входные данные — числа от 0 до 1, а сейчас они от 0 до 255. Исправим это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какой лосс использовать для классификации?\n",
    "\n",
    "Есть такой принцип максимального правдоподобия.\n",
    "\n",
    "Давайте максимизировать произведение вероятностей истинных событий. Звучит логично.\n",
    "\n",
    "$$ L = \\prod p_i $$\n",
    "\n",
    "Произведение оптимизировать очень не удобно. Давайте воспользуемся следующим трюком: давайте возьмем логарифм (любой, ведь все логарифмы отличаются в константу раз) и будем максимизировать сумму:\n",
    "\n",
    "$$ \\log L = \\log \\prod p_i = \\sum \\log p_i $$\n",
    "\n",
    "Эту штуку называют кроссэнтропией. Такое название пошло из теории информации, но нам пока знать это не надо.\n",
    "\n",
    "Для удобноства вместо чисел — от 0 до 9 — сконвертируем их в вектора размера 10, где будет стоять единица в нужном месте, и тогда функция потерь запишется так:\n",
    "\n",
    "...\n",
    "\n",
    "Такая кодировка называется one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = keras.utils.to_categorical(y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сохраним этот пайплайн. Он нам понадобится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно, вот в чем прелесть Keras: не нужно думать о размерах матриц, например."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(784,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть специальная функция, которая позволит проверить, что мы все сделали правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 476,490\n",
      "Trainable params: 476,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras оборачивает *статические* фреймворки. Им нужно скомпилировать сеть, заранее передав функцию потерь, оптимизатор и, опционально, желаемые метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 21s - loss: 0.5642 - acc: 0.8455 - val_loss: 0.2770 - val_acc: 0.9207\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 19s - loss: 0.2381 - acc: 0.9312 - val_loss: 0.1936 - val_acc: 0.9423\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 19s - loss: 0.1811 - acc: 0.9479 - val_loss: 0.1683 - val_acc: 0.9474\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 20s - loss: 0.1451 - acc: 0.9584 - val_loss: 0.1379 - val_acc: 0.9583\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 20s - loss: 0.1205 - acc: 0.9655 - val_loss: 0.1172 - val_acc: 0.9651\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 19s - loss: 0.1026 - acc: 0.9701 - val_loss: 0.1082 - val_acc: 0.9666\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 20s - loss: 0.0888 - acc: 0.9743 - val_loss: 0.0982 - val_acc: 0.9691\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 19s - loss: 0.0776 - acc: 0.9781 - val_loss: 0.0905 - val_acc: 0.9715\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 20s - loss: 0.0681 - acc: 0.9809 - val_loss: 0.0857 - val_acc: 0.9733\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 21s - loss: 0.0611 - acc: 0.9829 - val_loss: 0.0813 - val_acc: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c1f5dbf28>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'], c='r')\n",
    "plt.plot(history.history['val_acc'], c='ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте поиграться с параметрами сети. Попробуйте поставить другую функцию потерь, чтобы убедиться, что кроссэнтропия действительно лучше всех коррелирует с точностью."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
